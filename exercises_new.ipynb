{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "from jaxtyping import Float, Bool\n",
    "from torch import Tensor\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156b69e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# The numerical values of the maze correspond to the following:\n",
    "MAZE_WIDTH = 7\n",
    "\n",
    "MAZE_FINISH = -1\n",
    "MAZE_WALL = 0\n",
    "MAZE_EMPTY_SPACE = 1\n",
    "HARVESTABLE_CROP = 2\n",
    "HUMAN = 3\n",
    "\n",
    "\n",
    "MOVE_UP_IDX = 0\n",
    "MOVE_DOWN_IDX = 1\n",
    "MOVE_LEFT_IDX = 2\n",
    "MOVE_RIGHT_IDX = 3\n",
    "MOVES = {\n",
    "    (-1, 0): torch.tensor(MOVE_UP_IDX).to(device),  # up\n",
    "    (1, 0): torch.tensor(MOVE_DOWN_IDX).to(device),  # down\n",
    "    (0, -1): torch.tensor(MOVE_LEFT_IDX).to(device),  # left\n",
    "    (0, 1): torch.tensor(MOVE_RIGHT_IDX).to(device),  # right\n",
    "}\n",
    "\n",
    "\n",
    "def carve_path_in_maze(maze, starting_point):\n",
    "    moves = list(MOVES.keys())\n",
    "    starting_x, starting_y = starting_point\n",
    "    maze[starting_x, starting_y] = MAZE_EMPTY_SPACE\n",
    "    while True:\n",
    "        candidate_spaces_to_carve = []\n",
    "        for move in moves:\n",
    "            dx, dy = move\n",
    "            # We jump two moves ahead because otherwise you can end up creating\n",
    "            # \"caverns\" instead of only creating \"paths\"\n",
    "            # E.g. we might end up with something that looks like\n",
    "            # _____\n",
    "            # @@@__\n",
    "            # ____@\n",
    "            # ____@\n",
    "            # _____\n",
    "            #\n",
    "            # Instead of our desired (notice how we don't have a 4x4 gigantic\n",
    "            # empty space)\n",
    "            # _____\n",
    "            # @@@__\n",
    "            # ____@\n",
    "            # _@@@@\n",
    "            # _____\n",
    "            next_x = starting_x + dx\n",
    "            next_y = starting_y + dy\n",
    "            next_next_x = next_x + dx\n",
    "            next_next_y = next_y + dy\n",
    "            if 0 <= next_next_x < MAZE_WIDTH and \\\n",
    "                0 <= next_next_y < MAZE_WIDTH and \\\n",
    "                maze[next_next_x, next_next_y] == 0 and \\\n",
    "                maze[next_x, next_y] == 0:\n",
    "                    candidate_spaces_to_carve.append((next_x, next_y, next_next_x, next_next_y))\n",
    "        if not candidate_spaces_to_carve:\n",
    "            break\n",
    "        space_to_carve = random.choice(candidate_spaces_to_carve)\n",
    "        next_x, next_y, next_next_x, next_next_y = space_to_carve\n",
    "        maze[next_x, next_y], maze[next_next_x, next_next_y] = MAZE_EMPTY_SPACE, MAZE_EMPTY_SPACE\n",
    "        carve_path_in_maze(maze, (next_next_x, next_next_y))\n",
    "\n",
    "\n",
    "def add_exit(maze: Float[Tensor, \"maze_width maze_width\"]):\n",
    "    choices = (maze == MAZE_EMPTY_SPACE).nonzero().tolist()\n",
    "    furthest = max(choices, key=lambda x: x[0] + x[1])\n",
    "    maze[furthest[0], furthest[1]] = MAZE_FINISH\n",
    "\n",
    "\n",
    "def add_items_to_crannies_in_maze(maze: Float[Tensor, \"maze_width maze_width\"]):\n",
    "    all_empty_spaces = (maze == MAZE_EMPTY_SPACE).nonzero().tolist()\n",
    "    moves = list(MOVES.keys())\n",
    "    for (x, y) in all_empty_spaces:\n",
    "        if (x, y) == (0, 0):\n",
    "            continue\n",
    "        num_of_walls = 0\n",
    "        for move in moves:\n",
    "            dx, dy = move\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if nx < 0 or nx >= MAZE_WIDTH or ny < 0 or ny >= MAZE_WIDTH or maze[nx, ny] == MAZE_WALL:\n",
    "                num_of_walls += 1\n",
    "        if num_of_walls == 3:\n",
    "            maze[x, y] = random.choice((HARVESTABLE_CROP, HUMAN))\n",
    "\n",
    "\n",
    "def make_maze(maze_width: int) -> Float[Tensor, \"maze_width maze_width\"]:\n",
    "    maze = torch.zeros((maze_width, maze_width))\n",
    "    carve_path_in_maze(maze, (0, 0))\n",
    "    add_exit(maze)\n",
    "    add_items_to_crannies_in_maze(maze)\n",
    "    return maze\n",
    "\n",
    "def get_all_empty_spaces(maze: Float[Tensor, \"maze_width maze_width\"]) -> list[tuple[int, int]]:\n",
    "    return (maze == MAZE_EMPTY_SPACE).nonzero().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3153e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_maze(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e8f78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "HIT_WALL_PENALTY = -1\n",
    "# We'll rely entirely on our gamma decay to incentive fast pathing through the\n",
    "# maze.\n",
    "MOVE_PENALTY = 0\n",
    "WIN_REWARD = 10\n",
    "HARVEST_CROP_REWARD = 2\n",
    "HARVEST_HUMAN_PENALTY = -11\n",
    "\n",
    "def create_reward_tensor_from_maze(maze: torch.Tensor) -> torch.Tensor:\n",
    "    rewards = torch.zeros_like(maze)\n",
    "    # EXERCISE\n",
    "    # Add exercise section here\n",
    "    rewards[maze == MAZE_WALL] = HIT_WALL_PENALTY\n",
    "    rewards[maze == MAZE_EMPTY_SPACE] = MOVE_PENALTY\n",
    "    # raise NotImplementedException()\n",
    "    rewards[maze == HARVESTABLE_CROP] = HARVEST_CROP_REWARD\n",
    "    rewards[maze == HUMAN] = HARVEST_HUMAN_PENALTY\n",
    "    rewards[maze == MAZE_FINISH] = WIN_REWARD\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f80e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ReplayBuffer:\n",
    "    states: Float[Tensor, \"buffer input_size\"]\n",
    "    actions: Float[Tensor, \"buffer moves\"]\n",
    "    rewards: Float[Tensor, \"buffer\"]\n",
    "    is_terminals: Bool[Tensor, \"buffer\"]\n",
    "    next_states: Float[Tensor, \"buffer input_size\"]\n",
    "\n",
    "def get_reward(rewards, pos):\n",
    "    x, y = pos\n",
    "    a, b = rewards.shape\n",
    "    if 0 <= x < a and 0 <= y < b:\n",
    "        return rewards[x, y]\n",
    "    return HIT_WALL_PENALTY\n",
    "\n",
    "\n",
    "def get_maze():\n",
    "    maze = make_maze(MAZE_WIDTH)\n",
    "    rewards = create_reward_tensor_from_maze(maze)\n",
    "    return maze, rewards\n",
    "\n",
    "@dataclass\n",
    "class PostMoveInformation:\n",
    "    new_maze: torch.Tensor\n",
    "    new_pos: tuple[int, int]\n",
    "    reward: float\n",
    "    is_terminal: bool\n",
    "\n",
    "def get_reward(rewards, pos):\n",
    "    x, y = pos\n",
    "    a, b = rewards.shape\n",
    "    if 0 <= x < a and 0 <= y < b:\n",
    "        return rewards[x, y]\n",
    "    return HIT_WALL_PENALTY\n",
    "\n",
    "\n",
    "def get_next_pos(old_maze, rewards, position, move) -> PostMoveInformation:\n",
    "    is_terminal = True\n",
    "    new_pos = position  # default to forbidden move.\n",
    "    reward = HIT_WALL_PENALTY  # default to hitting a wall.\n",
    "    x, y = position\n",
    "    a, b = old_maze.shape\n",
    "    i, j = move\n",
    "    new_maze = old_maze\n",
    "    if 0 <= x + i < a and 0 <= y + j < b:\n",
    "        new_pos = (x + i, y + j)\n",
    "        reward = get_reward(rewards, new_pos)\n",
    "        is_terminal = old_maze[new_pos] == MAZE_FINISH or old_maze[new_pos] == MAZE_WALL\n",
    "\n",
    "        # Harvesting a crop (or a human!) consumes the tile and we get back an empty tile\n",
    "        if old_maze[new_pos] == HARVESTABLE_CROP or old_maze[new_pos] == HUMAN:\n",
    "            new_maze = torch.clone(old_maze)\n",
    "            new_maze[new_pos] = MAZE_EMPTY_SPACE\n",
    "\n",
    "    return PostMoveInformation(new_maze, new_pos, reward, is_terminal)\n",
    "\n",
    "def one_hot_encode_position(pos):\n",
    "    return F.one_hot(torch.tensor(pos).to(device), num_classes=MAZE_WIDTH).view(-1)\n",
    "\n",
    "def reshape_maze_and_position_to_input(maze, pos) -> Float[Tensor, \"input_size\"]:\n",
    "    wall_locations = maze == MAZE_WALL\n",
    "    crop_locations = maze == HARVESTABLE_CROP\n",
    "    human_locations = maze == HUMAN\n",
    "    return torch.cat((\n",
    "        wall_locations.view(-1),\n",
    "        crop_locations.view(-1),\n",
    "        human_locations.view(-1),\n",
    "        one_hot_encode_position(pos),\n",
    "    )).float()\n",
    "\n",
    "def create_replay_buffer(replay_buffer_size: int) -> ReplayBuffer:\n",
    "    states_buffer = torch.zeros((replay_buffer_size, INPUT_SIZE)).to(device)\n",
    "    actions_buffer = torch.zeros((replay_buffer_size, NUM_OF_MOVES)).to(device)\n",
    "    rewards_buffer = torch.zeros((replay_buffer_size)).to(device)\n",
    "    is_terminals_buffer = torch.zeros((replay_buffer_size), dtype=torch.bool).to(device)\n",
    "    next_states_buffer = torch.zeros((replay_buffer_size, INPUT_SIZE)).to(device)\n",
    "    old_maze, rewards = get_maze()\n",
    "    i = 0\n",
    "    exceeded_buffer_size = False\n",
    "    for pos in get_all_empty_spaces(old_maze):\n",
    "        if exceeded_buffer_size:\n",
    "            break\n",
    "        for mm in list(MOVES.keys()):\n",
    "            if i >= replay_buffer_size:\n",
    "                exceeded_buffer_size = True\n",
    "                break\n",
    "            move = mm\n",
    "            new_maze, new_pos, reward, is_terminal = dataclasses.astuple(get_next_pos(old_maze, rewards, pos, move))\n",
    "            states_buffer[i] = reshape_maze_and_position_to_input(old_maze, pos)\n",
    "            actions_buffer[i] = F.one_hot(MOVES[move], num_classes=NUM_OF_MOVES).to(device)\n",
    "            rewards_buffer[i] = reward\n",
    "            is_terminals_buffer[i] = is_terminal\n",
    "            next_states_buffer[i] = reshape_maze_and_position_to_input(new_maze, new_pos)\n",
    "            i += 1\n",
    "    return ReplayBuffer(states_buffer, actions_buffer, rewards_buffer, is_terminals_buffer, next_states_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_replay_buffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6685c2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyperparams\n",
    "\n",
    "# INPUT_SIZE consists of three copies of the maze, one for the base maze itself\n",
    "# and its walls, one for an overlay of crop locations, and one for an overlay of\n",
    "# human locations. We then include two one-hot encoded vectors of the current x\n",
    "# position and the current y position of the agent\n",
    "INPUT_SIZE = 3 * MAZE_WIDTH * MAZE_WIDTH + 2 * MAZE_WIDTH\n",
    "MAX_TRAINING_SET_SIZE = 2_000_000\n",
    "METHOD = 'exhaustive_search'\n",
    "GAMMA_DECAY = 0.95\n",
    "HIDDEN_SIZE = 2 * INPUT_SIZE\n",
    "EPOCH = 2\n",
    "BATCH_SIZE = 100_000\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_OF_MOVES = 4\n",
    "NUM_OF_STEPS_BEFORE_TARGET_UPDATE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de14573",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(HIDDEN_SIZE, NUM_OF_MOVES),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"... input_size\"]) -> Float[Tensor, \"... moves\"]:\n",
    "        q_values = self.linear_relu_stack(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18daed9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "class GameAgent:\n",
    "    def __init__(self, current_network: NeuralNetwork, target_network: NeuralNetwork):\n",
    "        self.current_network = current_network\n",
    "        self.target_network = target_network\n",
    "\n",
    "    def play_one_move_at_inference(self, maze: Float[Tensor, \"maze_width maze_width\"], pos: tuple[int, int]) -> tuple[int, int]:\n",
    "        input = reshape_maze_and_position_to_input(maze, pos)\n",
    "        q_values = self.current_network(input)\n",
    "        move = torch.argmax(q_values, dim=-1)\n",
    "        move_direction = list(MOVES.keys())[move]\n",
    "        return move_direction\n",
    "\n",
    "\n",
    "def train(game_agent: GameAgent):\n",
    "    replay_buffer = create_replay_buffer(MAX_TRAINING_SET_SIZE)\n",
    "    target_network = game_agent.target_network\n",
    "    current_network = game_agent.current_network\n",
    "    optimizer = torch.optim.AdamW(current_network.parameters(), lr=LEARNING_RATE)\n",
    "    i = 0 \n",
    "    for e in range(EPOCH):\n",
    "        print(f\"Epoch {e}\")\n",
    "        for _ in range(0, MAX_TRAINING_SET_SIZE, BATCH_SIZE):\n",
    "            states = replay_buffer.states[i:i+BATCH_SIZE]\n",
    "            actions = replay_buffer.actions[i:i+BATCH_SIZE]\n",
    "            rewards = replay_buffer.rewards[i:i+BATCH_SIZE]\n",
    "            is_terminals = replay_buffer.is_terminals[i:i+BATCH_SIZE]\n",
    "            next_states = replay_buffer.next_states[i:i+BATCH_SIZE]\n",
    "            with torch.no_grad():\n",
    "                max_target_q_values = target_network(next_states).max(dim=-1).values\n",
    "            max_target_q_values[is_terminals] = 0\n",
    "            target_q_values = rewards + GAMMA_DECAY * max_target_q_values\n",
    "            predictions = (current_network(states) * actions).sum(dim=-1)\n",
    "            loss = F.mse_loss(predictions, target_q_values)\n",
    "            print(f\"{loss=}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if i % NUM_OF_STEPS_BEFORE_TARGET_UPDATE == 0:\n",
    "                target_network.load_state_dict(current_network.state_dict())\n",
    "            i += 1\n",
    "\n",
    "\n",
    "# Implement negative reward for going back to a square you've been previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e94ee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "game_agent = GameAgent(NeuralNetwork(), NeuralNetwork())\n",
    "train(game_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808a217",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def string_repr_of_item(item):\n",
    "    if item == MAZE_WALL:\n",
    "        return ''\n",
    "    elif item == MAZE_EMPTY_SPACE:\n",
    "        return ''\n",
    "    elif item == HARVESTABLE_CROP:\n",
    "        return 'C'\n",
    "    elif item == HUMAN:\n",
    "        return 'H'\n",
    "    else:\n",
    "        return '?'\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_policy(model, maze):\n",
    "    dirs = {\n",
    "        0: '↑',\n",
    "        1: '↓',\n",
    "        2: '←',\n",
    "        3: '→',\n",
    "    }\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(-maze, 'Greys')\n",
    "    for pos_as_list in ((maze != MAZE_WALL) & (maze != MAZE_FINISH)).nonzero().tolist():\n",
    "        pos = tuple(pos_as_list)\n",
    "        q = model(reshape_maze_and_position_to_input(maze, pos))\n",
    "        action = int(torch.argmax(q).detach().cpu().item())\n",
    "        dir = dirs[action]\n",
    "        letter_label = string_repr_of_item(maze[pos].item())\n",
    "        ax.text(pos[1] - 0.3, pos[0] + 0.3, dir + letter_label)  # center arrows in empty slots\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = make_maze(7)\n",
    "plot_policy(game_agent.current_network, maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddc61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
