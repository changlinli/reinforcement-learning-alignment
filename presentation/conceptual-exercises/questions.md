# Preliminaries 

For these questions, unless otherwise specified, we assume that $Q$ means the
$Q$ function associated with the optimal policy.

# Hypothetical RL situation

Let's say we have an agent that we are training to be in control of a
self-driving car. It has a very simple reward function that is calculated in
discrete time steps. Every 10 seconds, the amount of miles that the car has
driven in those 10 seconds is the reward the agent gains. What are hypothetical
scenarios that might result in a high $R$ value, but a low $Q$ value? Feel free
to use real-world constraints (e.g. non-instantaneous acceleration, obstacles,
etc.) in your answer as you see fit.

# Analyzing the reward function

+ Based on the reward function of the maze agent, can you find an example where
  a given move has a low $R$ value, but a high $Q$ value?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is right next to the crop, choosing to harvest a crop has a lower
  $Q$ value than ignoring the crop?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is *not* next to the crop, choosing to pursue a path that results in
  harvesting the crop has a lower $Q$ value than ignoring that path?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is right next to a human, choosing to harvest the human has a higher
  $Q$ value than not harvesting the human?
+ Based on the reward function of the maze agent, is there any case where the
  agent has a higher $Q$ value by choosing to harvest a human vs choosing to not
  harvest a human?
+ **If your answer to the previous question is no, then why does the agent
  choose to harvest humans in some mazes in production even when it achieves low
  loss in training? If your answer to the previous question is yes, how would you
  modify the reward function to avoid this edge case?**

# Additional hint questions

The next questions are meant as hints to guide you towards solving the final
question of the previous section. **Only expand them on an individual basis if
you're having trouble solving the question.** I would recommend that you try to
think about the final question of the previous for at least 15 minutes before
opening these hints.

<details>
<summary>Hint Question 1</summary>
Are there any kinds of mazes that the current training code does not generate?
</details>

<details>
<summary>Hint Question 2</summary>
What guarantees do we have on the behavior of agents on mazes that are not
observed in its training set?
</details>

<details>
<summary>Hint Question 2</summary>
Are the mazes on which the agent goes to harvest humans present in the mazes
that are generated by the training code?
</details>


Additional questions:

You are training an AI agent to navigate a simplified grid world that consists of a starting point (S), empty spaces, a goal square (G) with a reward, and a trap square (T) with a negative reward. Each square on the grid represents a state. The agent can move up, down, left, or right when possible. The transition reward for reaching the goal is +10, for falling into the trap is -10, and for any other step is -1 (to encourage the shortest path). A discount factor γ of 0.9 is applied.

1. **Understanding Rewards:**
   - What is the immediate reward the agent receives when it moves from an empty square to the goal square?
   - How does the immediate reward for moving onto a trap square influence the agent’s behavior?
   - Can you design a reward function that encourages the agent to reach the goal square as quickly as possible without falling into the trap?

2. **Grasping Discounting:**
   - Explain how the discount factor γ affects the calculation of future rewards. What happens as γ approaches 1? What happens as γ approaches 0?
   - Describe how changing the discount factor to γ = 0.99 would affect the agent’s strategy compared to γ = 0.9.

3. **Understanding Value Functions (V):**
   - Define the state-value function V(s). What does it represent in the context of our grid world?
   - How would you estimate the value of the starting square (before learning begins) if the only information you have is the immediate reward for each action?

4. **Understanding Action-Value Functions (Q):**
   - Define the action-value function Q(s, a). How does it differ from the state-value function V(s)?
   - Given the scenario above, what might be the initial Q-values for the actions in the starting state?

5. **Exploring AI Misalignment:**
   - If the AI agent continues to receive a small positive reward for every step taken, how might this lead to a misalignment between the intended goal (reaching the goal square) and the agent's behavior?
   - Discuss a situation where the agent has learned a policy that collects a series of small rewards but never actually reaches the goal. What does this tell us about the importance of designing reward functions correctly?

6. **Policy and Value Iteration:**
   - Explain the difference between policy iteration and value iteration. Which might be more appropriate in a simple deterministic grid world like the one described here?
   - With the goal of teaching the concept of AI alignment, how would you use policy or value iteration to ensure that the agent reliably learns to reach the goal rather than falling into traps?

7. **Bellman Equations and Updates:**
   - Write down the Bellman equation for the state-value function V(s) for this grid world.
   - How would you use the Bellman equation to iteratively update the value of each state in the grid world?

8. **Exploration vs. Exploitation:**
   - Describe the exploration versus exploitation dilemma in the context of this reinforcement learning scenario.
   - Provide an example of a situation in this grid world where the agent might have to choose between exploring a new path or exploiting a known path that leads closer to the goal. How do different exploration strategies (e.g., ε-greedy) affect learning?
