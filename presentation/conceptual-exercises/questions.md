# Preliminaries 

For these questions, unless otherwise specified, we assume that $Q$ means the
$Q$ function associated with the optimal policy.

# Hypothetical RL situation

Let's say we have an agent that we are training to be in control of a
self-driving car. It has a very simple reward function that is calculated in
discrete time steps. Every 10 seconds, the amount of miles that the car has
driven in those 10 seconds is the reward the agent gains. What are hypothetical
scenarios that might result in a high $R$ value, but a low $Q$ value? Feel free
to use real-world constraints (e.g. non-instantaneous acceleration, obstacles,
etc.) in your answer as you see fit.

# Analyzing the reward function

+ Based on the reward function of the maze agent, can you find an example where
  a given move has a low $R$ value, but a high $Q$ value?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is right next to the crop, choosing to harvest a crop has a lower
  $Q$ value than ignoring the crop?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is *not* next to the crop, choosing to pursue a path that results in
  harvesting the crop has a lower $Q$ value than ignoring that path?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is right next to a human, choosing to harvest the human has a higher
  $Q$ value than not harvesting the human?
+ Based on the reward function of the maze agent, is there any case where the
  agent has a higher $Q$ value by choosing to harvest a human vs choosing to not
  harvest a human?
+ **If your answer to the previous question is no, then why does the agent
  choose to harvest humans in some mazes in production even when it achieves low
  loss in training? If your answer to the previous question is yes, how would you
  modify the reward function to avoid this edge case?**

# Additional hint questions

The next questions are meant as hints to guide you towards solving the final
question of the previous section. **Only expand them on an individual basis if
you're having trouble solving the question.** I would recommend that you try to
think about the final question of the previous for at least 15 minutes before
opening these hints.

<details>
<summary>Hint Question 1</summary>
Are there any kinds of mazes that the current training code does not generate?
</details>

<details>
<summary>Hint Question 2</summary>
What guarantees do we have on the behavior of agents on mazes that are not
observed in its training set?
</details>

<details>
<summary>Hint Question 2</summary>
Are the mazes on which the agent goes to harvest humans present in the mazes
that are generated by the training code?
</details>
