# Preliminaries 

For these questions, unless otherwise specified, we assume that $Q$ means the
$Q$ function associated with the optimal policy.

# Hypothetical RL situation

You are training an AI agent to play a simple game. It needs to navigate a grid
world that consists of a starting point (S), empty spaces, a goal square (G)
with a reward, and a trap square (T) with a negative reward. Each square on the
grid represents a state. The agent can move up, down, left, or right when
possible. The transition reward for reaching the goal is +10, for falling into
the trap is -10, and for any other step is -1 (to encourage the shortest path).
We'll call this last reward the "per-step reward." A discount factor $\gamma$ of
0.9 is applied.

The game ends immediately when the agent enters G or T.

Let's assume that our grid looks like the following:

```
S_T
___
__G
```

1. What is the immediate reward the agent receives when it is at `S` and moves
   one square down?
2. What is the Q-value of an optimal policy when the agent starts at `S` and
   moves one square down?
3. Again assuming we start at `S`, what is the path traced by one possible
   optimal policy before the game ends?
4. Let's assume that we try to play around with the per-step reward and
   discount factor. Is there any combination of per-step reward and discount
   factor that causes an agent with an optimal policy to never enter the goal
   square? Is there any combination of per-step reward and discount factor that
   causes an agent with a optimal policy to try to enter the trap square? 
5. Let's say that a genie tells you that the Q-function of the optimal policy
   given a position and a move is calculated by taking the Manhattan distance
   between the new position and G after making the move and then adding 10 to that
   value. The genie is wrong. Can you come up with a violation of Bellman's
   Equation to demonstrate to the genie that he's wrong?

# Analyzing the reward function

+ Based on the reward function of the maze agent, can you find an example where
  a given move has a low $R$ value, but a high $Q$ value?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is right next to the crop, choosing to harvest a crop has a lower
  $Q$ value than ignoring the crop?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is *not* next to the crop, choosing to pursue a path that results in
  harvesting the crop has a lower $Q$ value than ignoring that path?
+ Based on the reward function of the maze agent, is there any case where, if
  the agent is right next to a human, choosing to harvest the human has a higher
  $Q$ value than not harvesting the human?
+ Based on the reward function of the maze agent, is there any case where the
  agent has a higher $Q$ value by choosing to harvest a human vs choosing to not
  harvest a human?
+ **If your answer to the previous question is no, then why does the agent
  choose to harvest humans in some mazes in production even when it achieves low
  loss in training? If your answer to the previous question is yes, how would you
  modify the reward function to avoid this edge case?**

# Additional hint questions

The next questions are meant as hints to guide you towards solving the final
question of the previous section. **Only expand them on an individual basis if
you're having trouble solving the question.** I would recommend that you try to
think about the final question of the previous for at least 15 minutes before
opening these hints.

<details>
<summary>Hint Question 1</summary>
Are there any kinds of mazes that the current training code does not generate?
</details>

<details>
<summary>Hint Question 2</summary>
What guarantees do we have on the behavior of agents on mazes that are not
observed in its training set?
</details>

<details>
<summary>Hint Question 2</summary>
Are the mazes on which the agent goes to harvest humans present in the mazes
that are generated by the training code?
</details>


Additional questions:

You are training an AI agent to play a simple game. It needs to navigate a grid
world that consists of a starting point (S), empty spaces, a goal square (G)
with a reward, and a trap square (T) with a negative reward. Each square on the
grid represents a state. The agent can move up, down, left, or right when
possible. The transition reward for reaching the goal is +10, for falling into
the trap is -10, and for any other step is -1 (to encourage the shortest path).
We'll call this last reward the "per-step reward." A discount factor $\gamma$ of
0.9 is applied.

The game ends immediately when the agent enters G or T.

Let's assume that our grid looks like the following:

```
S_T
___
__G
```

1. What is the immediate reward the agent receives when it is at `S` and moves
   one square down?
2. What is the Q-value of an optimal policy when the agent starts at `S` and
   moves one square down?
3. Again assuming we start at `S`, what is the path traced by one possible
   optimal policy before the game ends?
4. Let's assume that we try to play around with the per-step reward and
   discount factor. Is there any combination of per-step reward and discount
   factor that causes an agent with an optimal policy to never enter the goal
   square? Is there any combination of per-step reward and discount factor that
   causes an agent with a optimal policy to try to enter the trap square? 
5. Let's say that a genie tells you that the Q-function of the optimal policy
   given a position and a move is calculated by taking the Manhattan distance
   between the new position and G after making the move and then adding 10 to that
   value. The genie is wrong. Can you come up with a violation of Bellman's
   Equation to demonstrate to the genie that he's wrong?
