<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Changlin Li (mail@changlinli.com)" />
  <title>Reinforcement Learning</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Reinforcement Learning</h1>
  <p class="author">
Changlin Li (mail@changlinli.com)
  </p>
</div>
<div id="what-is-reinforcement-learning" class="slide section level1">
<h1>What is Reinforcement Learning</h1>
<ul>
<li>An agent that can be in any one of a variety of states that must
perform actions in an environment</li>
<li>Train using short-term rewards based on agent state and behavior to
guide long-term behavior</li>
<li>Human designer does not know the optimal long-term strategy</li>
</ul>
</div>
<div
id="how-does-it-differ-from-standard-supervisedunsupervise-learning"
class="slide section level1">
<h1>How does it differ from standard supervised/unsupervise
learning</h1>
<ul>
<li>Supervised learning requires labeling with known good behavior
<ul>
<li>In an RL context humans may not know what the optimal strategy is
and so can’t generate optimal labels</li>
</ul></li>
<li>Unsupervised learning is generally descriptive
<ul>
<li>Way of visualizing/understanding data, e.g. clustering</li>
<li>Doesn’t come up with strategies on its own</li>
<li>Want an agent that is capable of probing</li>
</ul></li>
</ul>
</div>
<div id="example" class="slide section level1">
<h1>Example</h1>
<ul>
<li>Game of chess</li>
<li>Humans don’t know what the optimal long-term straegy is</li>
<li>But we have a good idea of what immediate rewards should look like
<ul>
<li>E.g. if you checkmate your opponent on the next move you should get
very high reward for that move</li>
<li>Likewise if you can get checkmated on the next move by playing a
certain move you should get very low reward for that move</li>
</ul></li>
<li>Can use intermediate rewards such as taking pieces</li>
</ul>
</div>
<div id="simpler-example" class="slide section level1">
<h1>Simpler example</h1>
<ul>
<li>Mazes</li>
<li>RL is a bit of overkill here because humans do know what the
optimal</li>
</ul>
</div>
<div id="basic-rl-strategy-is-q-learning" class="slide section level1">
<h1>Basic RL strategy is Q-learning</h1>
<ul>
<li>We provide R (short-term reward function)</li>
<li>Q is the optimal reward function, i.e. it is the maximal possible
long-term reward for performing a certain action</li>
<li>Q can be defined recursively
<ul>
<li>`Q(s, a) = R(s, a) + max_a_i(Q(s’, a_i))</li>
<li>Known as Bellman’s Equation</li>
</ul></li>
</ul>
</div>
<div id="example-r-and-q-in-maze" class="slide section level1">
<h1>Example R and Q in maze</h1>
<ul>
<li>State is a coordinate in the maze</li>
<li>Actions are up, down, right, left</li>
<li>R(s, a) = 1000 if action a gets you to the exit on your next move,
zero otherwise</li>
</ul>
</div>
<div id="example-r-and-q-in-maze-1" class="slide section level1">
<h1>Example R and Q in maze</h1>
<p>We are at (1, 1) in a maze and we want to calculate Q for the action
UP</p>
<ul>
<li>R((1, 1), UP) = 0 (assuming exit is not at (1, 2))</li>
<li>So Q((1, 1), UP) = 0 + max(Q((1, 2), UP), Q((1, 2), DOWN), Q((1, 2),
LEFT), Q((1, 2), RIGHT))</li>
</ul>
</div>
<div id="this-example-is-a-bit-trivial" class="slide section level1">
<h1>This example is a bit trivial</h1>
<ul>
<li>Since R is always 0 or 1000 and you can always eventually get to the
exit from anywhere in the maze, Q(s, a) is always 1000</li>
</ul>
</div>
<div id="this-is-bad-for-agent-training" class="slide section level1">
<h1>This is bad for agent training</h1>
<ul>
<li>The agent will probably learn to go in circles, because no incentive
to go to exit quickly</li>
</ul>
</div>
<div id="add-small-negative-reward-penalty-for-every-move"
class="slide section level1">
<h1>Add small negative reward (penalty) for every move</h1>
<ul>
<li>R(s, a) = 1000 if a gets you to the exit on the next move</li>
<li>Otherwise -1</li>
</ul>
</div>
<div id="lets-go-to-code" class="slide section level1">
<h1>Let’s go to code!</h1>
</div>
</body>
</html>
