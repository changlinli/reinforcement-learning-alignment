{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We are training an RL agent to navigate arbitrary 7x7 mazes. The agent always starts in the upper left-hand corner\n",
    "# and the exit of the maze is always in the lower right-hand corner. Along the way the agent can also harvest items in\n",
    "# the maze. There are two kinds of items it can harvest: crops and humans. We have a mild preference for the agent to\n",
    "# harvest crops. We *definitely* don't want the agent to harvest humans!\n",
    "#\n",
    "# However, we find that the agent, while seeming to be perfectly fine during\n",
    "# training, goes off the rails in production and starts harvesting humans! Our\n",
    "# job will be to find why that happens.\n",
    "#\n",
    "# Note that there are a couple conditions to this game, which make it a bit\n",
    "# easier to train our agent.\n",
    "# 1. There is always a way out of the maze\n",
    "# 2. The starting point and exit point of the maze will always be the same\n",
    "# 3. Harvesting something just involves moving onto the square containing the\n",
    "# crop or the human. Once harvested, that thing will disappear from the maze.\n",
    "# 4. The crops and humans will never \"block\" the path through the maze. That is\n",
    "# there will always be a path from the start to end of the maze which avoids all\n",
    "# crops and humans\n",
    "\n",
    "# Some preliminary imports that we'll be using\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "\n",
    "# Some nice preliminary functions for testing.\n",
    "\n",
    "def assert_with_expect(expected, actual):\n",
    "    assert expected == actual, f\"Expected: {expected} Actual: {actual}\"\n",
    "\n",
    "\n",
    "def assert_list_of_floats_within_epsilon(\n",
    "    expected: list[float], \n",
    "    actual: list[float],\n",
    "    eps=0.0001,\n",
    "):\n",
    "    if len(expected) != len(actual):\n",
    "        raise AssertionError(f\"Expected: {expected} Actual: {actual}\")\n",
    "    is_within_eps = True\n",
    "    for e, a in zip(expected, actual):\n",
    "        is_within_eps = is_within_eps and abs(e - a) < eps\n",
    "    if not is_within_eps:\n",
    "        raise AssertionError(f\"Expected: {expected} Actual: {actual}\")\n",
    "\n",
    "\n",
    "def assert_tensors_within_epsilon(\n",
    "    expected: torch.Tensor,\n",
    "    actual: torch.Tensor,\n",
    "    eps=0.001,\n",
    "):\n",
    "    if expected.shape != actual.shape:\n",
    "        raise AssertionError(f\"Shapes of tensors do not match! Expected: {expected.shape} Acutal: {actual.shape}\")\n",
    "    differences_within_epsilon = abs(expected - actual) < eps\n",
    "    if not differences_within_epsilon.all():\n",
    "        raise AssertionError(f\"Values of tensors do not match! Expected: {expected} Actual: {actual}\")\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895dfdff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# An example of the 7x7 maze looks like the following\n",
    "\n",
    "example_maze = torch.tensor([\n",
    "    [1, 0, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 2, 0, 1],\n",
    "    [1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 0, 0],\n",
    "    [3, 1, 1, 1, 1, 1, -1],\n",
    "])\n",
    "\n",
    "# The numerical values of the maze correspond to the following:\n",
    "\n",
    "MAZE_FINISH = -1\n",
    "MAZE_WALL = 0\n",
    "MAZE_EMPTY_SPACE = 1\n",
    "HARVESTABLE_CROP = 2\n",
    "HUMAN = 3\n",
    "\n",
    "# This will be a useful constant since all our mazes will be 7x7.\n",
    "\n",
    "MAZE_WIDTH = 7\n",
    "\n",
    "# First let's make sure that we understand exactly how our maze is set up.\n",
    "# EXERCISE: Can you give the coordinates that indicate where the HARVESTABLE_CROP is?\n",
    "\n",
    "first_coord = 2\n",
    "second_coord = 4\n",
    "\n",
    "value_of_example_maze_at_x_y = example_maze[first_coord, second_coord]\n",
    "\n",
    "assert_with_expect(expected=HARVESTABLE_CROP, actual=value_of_example_maze_at_x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc692fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ffbb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here is some code that is useful for visualizing the mazes so that we can\n",
    "# understand how the agent navigates the maze. Let's first start off with a very\n",
    "# simple ASCII approximation.\n",
    "\n",
    "# Make sure to take a second to look at the ASCII representation and see if it\n",
    "# aligns what you expected the maze to look like.\n",
    "\n",
    "def ascii_maze(maze):\n",
    "    lookup = {MAZE_WALL: '@', MAZE_EMPTY_SPACE: '_', MAZE_FINISH: 'x', HUMAN: 'h', HARVESTABLE_CROP: 'c'}\n",
    "    print('\\n'.join(''.join(lookup[i] for i in row) for row in maze.tolist()))\n",
    "\n",
    "ascii_maze(example_maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f15218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# But an ASCII maze isn't that fun to look at. Let's get a better visualization.\n",
    "\n",
    "def string_repr_of_item(item):\n",
    "    if item == MAZE_WALL:\n",
    "        return ''\n",
    "    elif item == MAZE_EMPTY_SPACE:\n",
    "        return ''\n",
    "    elif item == HARVESTABLE_CROP:\n",
    "        return 'C'\n",
    "    elif item == HUMAN:\n",
    "        return 'H'\n",
    "    else:\n",
    "        return '?'\n",
    "\n",
    "\n",
    "def plot_maze(maze, label_items_with_letters = True):\n",
    "    maze_width = len(maze[0])\n",
    "    _, ax = plt.subplots()\n",
    "    ax.imshow(-maze, 'Greys')\n",
    "    plt.imshow(-maze, 'Greys')\n",
    "    if label_items_with_letters:\n",
    "        for (x, y) in [(x, y) for x in range(0, maze_width) for y in range(0, maze_width)]:\n",
    "            ax.text(y - 0.3, x + 0.3, string_repr_of_item(maze[x, y].item()))\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_maze(example_maze)\n",
    "\n",
    "INPUT_SIZE = 4 * MAZE_WIDTH * MAZE_WIDTH + 2 * MAZE_WIDTH\n",
    "MOVE_UP_IDX = 0\n",
    "MOVE_DOWN_IDX = 1\n",
    "MOVE_LEFT_IDX = 2\n",
    "MOVE_RIGHT_IDX = 3\n",
    "MOVES = {\n",
    "    (-1, 0): torch.tensor(MOVE_UP_IDX).to(device),  # up\n",
    "    (1, 0): torch.tensor(MOVE_DOWN_IDX).to(device),  # down\n",
    "    (0, -1): torch.tensor(MOVE_LEFT_IDX).to(device),  # left\n",
    "    (0, 1): torch.tensor(MOVE_RIGHT_IDX).to(device),  # right\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's actually make the function that will make the agent play the game\n",
    "\n",
    "HIT_WALL_PENALTY = -1\n",
    "# We'll rely entirely on our gamma decay to incentive fast pathing through the\n",
    "# maze.\n",
    "MOVE_PENALTY = 0\n",
    "WIN_REWARD = 10\n",
    "HARVEST_CROP_REWARD = 2\n",
    "HARVEST_HUMAN_PENALTY = -11\n",
    "\n",
    "\n",
    "def create_reward_tensor_from_maze(maze: torch.Tensor) -> torch.Tensor:\n",
    "    rewards = torch.zeros_like(maze)\n",
    "    # EXERCISE\n",
    "    # Add exercise section here\n",
    "    rewards[maze == MAZE_WALL] = HIT_WALL_PENALTY\n",
    "    rewards[maze == MAZE_EMPTY_SPACE] = MOVE_PENALTY\n",
    "    # raise NotImplementedException()\n",
    "    rewards[maze == HARVESTABLE_CROP] = HARVEST_CROP_REWARD\n",
    "    rewards[maze == HUMAN] = HARVEST_HUMAN_PENALTY\n",
    "    rewards[maze == MAZE_FINISH] = WIN_REWARD\n",
    "    return rewards\n",
    "\n",
    "\n",
    "expected_reward_tensor_for_example_maze = torch.tensor([\n",
    "    [  0,  -1,   0,   0,   0,   0,   0],\n",
    "    [  0,  -1,   0,  -1,  -1,  -1,   0],\n",
    "    [  0,  -1,   0,  -1,   2,  -1,   0],\n",
    "    [  0,  -1,   0,  -1,   0,  -1,   0],\n",
    "    [  0,   0,   0,  -1,   0,   0,   0],\n",
    "    [ -1,  -1,  -1,  -1,   0,  -1,  -1],\n",
    "    [-11,   0,   0,   0,   0,   0,  10],\n",
    "])\n",
    "\n",
    "assert_tensors_within_epsilon(\n",
    "    expected=expected_reward_tensor_for_example_maze,\n",
    "    actual=create_reward_tensor_from_maze(example_maze),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_reward(rewards, pos):\n",
    "    x, y = pos\n",
    "    a, b = rewards.shape\n",
    "    if 0 <= x < a and 0 <= y < b:\n",
    "        return rewards[x, y]\n",
    "    return HIT_WALL_PENALTY\n",
    "\n",
    "\n",
    "def get_maze():\n",
    "    # maze = default_maze\n",
    "    maze = make_maze(MAZE_WIDTH)\n",
    "    rewards = create_reward_tensor_from_maze(maze)\n",
    "    return maze, rewards\n",
    "\n",
    "MOVE_UP_IDX = 0\n",
    "MOVE_DOWN_IDX = 1\n",
    "MOVE_LEFT_IDX = 2\n",
    "MOVE_RIGHT_IDX = 3\n",
    "MOVES = {\n",
    "    (-1, 0): torch.tensor(MOVE_UP_IDX).to(device),  # up\n",
    "    (1, 0): torch.tensor(MOVE_DOWN_IDX).to(device),  # down\n",
    "    (0, -1): torch.tensor(MOVE_LEFT_IDX).to(device),  # left\n",
    "    (0, 1): torch.tensor(MOVE_RIGHT_IDX).to(device),  # right\n",
    "}\n",
    "\n",
    "MOVE_UP = (-1, 0)\n",
    "MOVE_DOWN = (1, 0)\n",
    "MOVE_LEFT = (0, -1)\n",
    "MOVE_RIGHT = (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5274e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PostMoveInformation:\n",
    "    new_maze: torch.Tensor\n",
    "    new_pos: tuple[int, int]\n",
    "    reward: float\n",
    "    is_terminal: bool\n",
    "\n",
    "def get_next_pos(old_maze, rewards, position, move) -> PostMoveInformation:\n",
    "    \"\"\"\n",
    "    This function takes in a maze, a PyTorch tensor of rewards associated with that maze, the current position of the agent within the maze, and then\n",
    "    \"\"\"\n",
    "    is_terminal = True\n",
    "    new_pos = position  # default to forbidden move.\n",
    "    reward = HIT_WALL_PENALTY  # default to hitting a wall.\n",
    "    x, y = position\n",
    "    a, b = old_maze.shape\n",
    "    i, j = move\n",
    "    new_maze = old_maze\n",
    "    if 0 <= x + i < a and 0 <= y + j < b:\n",
    "        new_pos = (x + i, y + j)\n",
    "        reward = get_reward(rewards, new_pos)\n",
    "        is_terminal = old_maze[new_pos] == MAZE_FINISH or old_maze[new_pos] == MAZE_WALL\n",
    "\n",
    "        # Harvesting a crop (or a human!) consumes the tile and we get back an empty tile\n",
    "        if old_maze[new_pos] == HARVESTABLE_CROP or old_maze[new_pos] == HUMAN:\n",
    "            new_maze = torch.clone(old_maze)\n",
    "            new_maze[new_pos] = MAZE_EMPTY_SPACE\n",
    "\n",
    "    return PostMoveInformation(new_maze, new_pos, reward, is_terminal)\n",
    "\n",
    "\n",
    "def assert_post_move_informations_are_equal(expected: PostMoveInformation, actual: PostMoveInformation):\n",
    "    assert_tensors_within_epsilon(\n",
    "        expected=expected.new_maze, \n",
    "        actual=actual.new_maze\n",
    "    )\n",
    "    assert_with_expect(\n",
    "        expected=expected.new_pos, \n",
    "        actual=actual.new_pos\n",
    "    )\n",
    "    assert_with_expect(\n",
    "        expected=expected.reward, \n",
    "        actual=actual.reward\n",
    "    )\n",
    "    assert_with_expect(\n",
    "        expected=expected.is_terminal, \n",
    "        actual=actual.is_terminal\n",
    "    )\n",
    "\n",
    "result_of_move_from_start = get_next_pos(example_maze, create_reward_tensor_from_maze(example_maze), (0, 0), MOVE_DOWN)\n",
    "expected_answer = PostMoveInformation(\n",
    "    new_maze=torch.tensor([\n",
    "        [ 1,  0,  1,  1,  1,  1,  1],\n",
    "        [ 1,  0,  1,  0,  0,  0,  1],\n",
    "        [ 1,  0,  1,  0,  2,  0,  1],\n",
    "        [ 1,  0,  1,  0,  1,  0,  1],\n",
    "        [ 1,  1,  1,  0,  1,  1,  1],\n",
    "        [ 0,  0,  0,  0,  1,  0,  0],\n",
    "        [ 3,  1,  1,  1,  1,  1, -1]]),\n",
    "    new_pos=(1, 0),\n",
    "    reward=torch.tensor(0),\n",
    "    is_terminal=torch.tensor(False)\n",
    ")\n",
    "assert_post_move_informations_are_equal(expected=expected_answer, actual=result_of_move_from_start)\n",
    "\n",
    "# Make sure that if we hit a wall that our game ends\n",
    "result_of_hitting_wall = get_next_pos(result_of_move_from_start.new_maze, create_reward_tensor_from_maze(result_of_move_from_start.new_maze), (1, 0), MOVE_RIGHT)\n",
    "assert_with_expect(expected=torch.tensor(True), actual=result_of_hitting_wall.is_terminal)\n",
    "\n",
    "# Make sure that if we harvest a crop that it disappears and is replaced by an\n",
    "# empty space + we get the reward that we expect\n",
    "result_of_move_from_start = get_next_pos(example_maze, create_reward_tensor_from_maze(example_maze), (3, 4), MOVE_UP)\n",
    "expected_answer = PostMoveInformation(\n",
    "    new_maze=torch.tensor([\n",
    "        [ 1,  0,  1,  1,  1,  1,  1],\n",
    "        [ 1,  0,  1,  0,  0,  0,  1],\n",
    "        [ 1,  0,  1,  0,  1,  0,  1],\n",
    "        [ 1,  0,  1,  0,  1,  0,  1],\n",
    "        [ 1,  1,  1,  0,  1,  1,  1],\n",
    "        [ 0,  0,  0,  0,  1,  0,  0],\n",
    "        [ 3,  1,  1,  1,  1,  1, -1]]),\n",
    "    new_pos=(2, 4),\n",
    "    reward=torch.tensor(2),\n",
    "    is_terminal=torch.tensor(False)\n",
    ")\n",
    "assert_post_move_informations_are_equal(expected=expected_answer, actual=result_of_move_from_start)\n",
    "\n",
    "# Make sure that if we harvest a human that it disappears and is replaced by an\n",
    "# empty space + we get the reward that we expect\n",
    "result_of_move_from_start = get_next_pos(example_maze, create_reward_tensor_from_maze(example_maze), (6, 1), MOVE_LEFT)\n",
    "expected_answer = PostMoveInformation(\n",
    "    new_maze=torch.tensor([\n",
    "        [ 1,  0,  1,  1,  1,  1,  1],\n",
    "        [ 1,  0,  1,  0,  0,  0,  1],\n",
    "        [ 1,  0,  1,  0,  2,  0,  1],\n",
    "        [ 1,  0,  1,  0,  1,  0,  1],\n",
    "        [ 1,  1,  1,  0,  1,  1,  1],\n",
    "        [ 0,  0,  0,  0,  1,  0,  0],\n",
    "        [ 1,  1,  1,  1,  1,  1, -1]]),\n",
    "    new_pos=(6, 0),\n",
    "    reward=torch.tensor(-11),\n",
    "    is_terminal=torch.tensor(False)\n",
    ")\n",
    "assert_post_move_informations_are_equal(expected=expected_answer, actual=result_of_move_from_start)\n",
    "\n",
    "# Make sure that getting to the end of the maze gives us the reward we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07d6f4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now let's go ahead and make the code that goes and calculates how to generate\n",
    "# test mazes for our training. Make sure that you understand carve_path_in_maze\n",
    "# well since it's the function that is primarily responsible for generating the\n",
    "# mazes we will use for training!\n",
    "\n",
    "# We want to \n",
    "def carve_path_in_maze(maze, starting_point):\n",
    "    moves = list(MOVES.keys())\n",
    "    starting_x, starting_y = starting_point\n",
    "    maze[starting_x, starting_y] = MAZE_EMPTY_SPACE\n",
    "    while True:\n",
    "        candidate_spaces_to_carve = []\n",
    "        for move in moves:\n",
    "            dx, dy = move\n",
    "            # We jump two moves ahead because otherwise you can end up creating\n",
    "            # \"caverns\" instead of only creating \"paths\"\n",
    "            # E.g. we might end up with something that looks like\n",
    "            # _____\n",
    "            # @@@__\n",
    "            # ____@\n",
    "            # ____@\n",
    "            # _____\n",
    "            #\n",
    "            # Instead of our desired (notice how we don't have a 4x4 gigantic\n",
    "            # empty space)\n",
    "            # _____\n",
    "            # @@@__\n",
    "            # ____@\n",
    "            # _@@@@\n",
    "            # _____\n",
    "            next_x = starting_x + dx\n",
    "            next_y = starting_y + dy\n",
    "            next_next_x = next_x + dx\n",
    "            next_next_y = next_y + dy\n",
    "            if 0 <= next_next_x < MAZE_WIDTH and \\\n",
    "                0 <= next_next_y < MAZE_WIDTH and \\\n",
    "                maze[next_next_x, next_next_y] == 0 and \\\n",
    "                maze[next_x, next_y] == 0:\n",
    "                    candidate_spaces_to_carve.append((next_x, next_y, next_next_x, next_next_y))\n",
    "        if not candidate_spaces_to_carve:\n",
    "            break\n",
    "        space_to_carve = random.choice(candidate_spaces_to_carve)\n",
    "        next_x, next_y, next_next_x, next_next_y = space_to_carve\n",
    "        maze[next_x, next_y], maze[next_next_x, next_next_y] = MAZE_EMPTY_SPACE, MAZE_EMPTY_SPACE\n",
    "        carve_path_in_maze(maze, (next_next_x, next_next_y))\n",
    "\n",
    "example_maze_to_carve = torch.zeros((MAZE_WIDTH, MAZE_WIDTH))\n",
    "\n",
    "# Let's go over an example of the maze\n",
    "plot_maze(example_maze_to_carve, MAZE_WIDTH)\n",
    "\n",
    "carve_path_in_maze(example_maze_to_carve, (0, 0))\n",
    "\n",
    "plot_maze(example_maze_to_carve, MAZE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c2df5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# We then add an exit to the maze, which is always in the same place, to make\n",
    "# this task easier for an agent to learn.\n",
    "\n",
    "def add_exit(maze):\n",
    "    choices = (maze == MAZE_EMPTY_SPACE).nonzero().tolist()\n",
    "    furthest = max(choices, key=lambda x: x[0] + x[1])\n",
    "    maze[furthest[0], furthest[1]] = MAZE_FINISH\n",
    "\n",
    "add_exit(example_maze_to_carve)\n",
    "plot_maze(example_maze_to_carve, MAZE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9827f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Next we add items to the maze.\n",
    "\n",
    "def add_items_to_crannies_in_maze(maze):\n",
    "    all_empty_spaces = (maze == MAZE_EMPTY_SPACE).nonzero().tolist()\n",
    "    moves = list(MOVES.keys())\n",
    "    for (x, y) in all_empty_spaces:\n",
    "        if (x, y) == (0, 0):\n",
    "            continue\n",
    "        num_of_walls = 0\n",
    "        for move in moves:\n",
    "            dx, dy = move\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if nx < 0 or nx >= MAZE_WIDTH or ny < 0 or ny >= MAZE_WIDTH or maze[nx, ny] == MAZE_WALL:\n",
    "                num_of_walls += 1\n",
    "        if num_of_walls == 3:\n",
    "            maze[x, y] = random.choice((HARVESTABLE_CROP, HUMAN))\n",
    "\n",
    "add_items_to_crannies_in_maze(example_maze_to_carve)\n",
    "plot_maze(example_maze_to_carve, MAZE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50469dae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_maze(width):\n",
    "    maze = torch.zeros((width, width))\n",
    "    carve_path_in_maze(maze, (0, 0))\n",
    "    add_exit(maze)\n",
    "    add_items_to_crannies_in_maze(maze)\n",
    "    return maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here's the crucial question!\n",
    "# Remember again when answering these questions, we are guaranteed that every\n",
    "# maze in production has a viable path from the start to the end without getting\n",
    "# blocked by crops or humans.\n",
    "\n",
    "# If there is at least one move that does not involve harvesting a human, will\n",
    "# the optimal policy ever harvest a human?\n",
    "\n",
    "# EXERCISE\n",
    "# Dummy NotImplementedError\n",
    "# Once you've come up with an answer you can just delete this\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac829d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's actually create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda05c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparams\n",
    "MAX_TRAINING_SET_SIZE = 20\n",
    "METHOD = 'exhaustive_search'\n",
    "GAMMA_DECAY = 0.95\n",
    "HIDDEN_SIZE = 2 * INPUT_SIZE\n",
    "EPOCH = 20\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(HIDDEN_SIZE, len(MOVES)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch_randomized():\n",
    "    batch = []\n",
    "    old_maze, rewards = get_maze()\n",
    "    positions = random.choices((old_maze == 1).nonzero().tolist(), k=BATCH_SIZE)\n",
    "    for pos in positions:\n",
    "        move = random.choice(list(MOVES.keys()))\n",
    "        new_maze, new_pos, reward, is_terminal = dataclasses.astuple(get_next_pos(old_maze, rewards, pos, move))\n",
    "        batch.append((old_maze, pos, move, new_maze, new_pos, reward, is_terminal))\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_batch_exhaustive_search():\n",
    "    batch = []\n",
    "    old_maze, rewards = get_maze()\n",
    "    for pos in (old_maze == 1).nonzero().tolist():\n",
    "        for mm in list(MOVES.keys()):\n",
    "            move = mm\n",
    "            new_maze, new_pos, reward, is_terminal = dataclasses.astuple(get_next_pos(old_maze, rewards, pos, move))\n",
    "            batch.append((old_maze, pos, move, new_maze, new_pos, reward, is_terminal))\n",
    "    return batch\n",
    "\n",
    "\n",
    "def one_hot_encode_position(pos):\n",
    "    return F.one_hot(torch.tensor(pos).to(device), num_classes=MAZE_WIDTH).view(-1)\n",
    "\n",
    "\n",
    "def to_input(maze, pos):\n",
    "    wall_locations = maze == MAZE_WALL\n",
    "    crop_locations = maze == HARVESTABLE_CROP\n",
    "    human_locations = maze == HUMAN\n",
    "    finish_locations = maze == MAZE_FINISH\n",
    "    return torch.cat((\n",
    "        wall_locations.view(-1),\n",
    "        crop_locations.view(-1),\n",
    "        human_locations.view(-1),\n",
    "        finish_locations.view(-1),\n",
    "        one_hot_encode_position(pos),\n",
    "    )).float()\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    losses = []\n",
    "    training_set = deque([], maxlen=MAX_TRAINING_SET_SIZE)\n",
    "    for epoch in range(EPOCH):\n",
    "        new_batch = get_batch_exhaustive_search()\n",
    "        training_set.append(new_batch)\n",
    "        for batch in training_set:\n",
    "            # train vectorized\n",
    "            old_states, moves, new_states, rewards, terminal = [], [], [], [], []\n",
    "            for old_maze, pos, move, new_maze, new_pos, reward, is_terminal in batch:\n",
    "                old_states.append(to_input(old_maze, pos))\n",
    "                moves.append(F.one_hot(MOVES[move], num_classes=len(MOVES)))\n",
    "                new_states.append(to_input(new_maze, new_pos))\n",
    "                rewards.append(reward)\n",
    "                terminal.append(0. if is_terminal else 1.)  # no Q'(s', a') if terminal state\n",
    "\n",
    "            old_states_stacked = torch.stack(old_states).to(device)\n",
    "            moves_stacked = torch.stack(moves).to(device)\n",
    "            new_states_stacked = torch.stack(new_states).to(device)\n",
    "            rewards_stacked = torch.tensor(rewards).to(device).view(-1, 1)\n",
    "            TERMINAL = torch.tensor(terminal).to(device).view(-1, 1)\n",
    "            bellman_left = (model(old_states_stacked) * moves_stacked).sum(dim=1, keepdim=True)\n",
    "            qqs = model(new_states_stacked).max(dim=1, keepdim=True).values\n",
    "            bellman_right = rewards_stacked + qqs * TERMINAL * GAMMA_DECAY\n",
    "\n",
    "            loss = F.mse_loss(bellman_left, bellman_right)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"epoch: {epoch: 5} loss: {torch.tensor(losses).mean():.8f}\")\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac483141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_to_move = {i.detach().item(): v for v, i in MOVES.items()}\n",
    "\n",
    "def play(model, maze, pos=(0, 0)):\n",
    "    depth = 1000\n",
    "    move_list = []\n",
    "    while True:\n",
    "        qs = model(to_input(maze, pos))\n",
    "        move = idx_to_move[qs.argmax().tolist()]\n",
    "        new_pos = (pos[0] + move[0], pos[1] + move[1])\n",
    "        move_list.append((move, pos, new_pos))\n",
    "        if 0 <= new_pos[0] < MAZE_WIDTH and 0 <= new_pos[1] < MAZE_WIDTH:\n",
    "            pos = new_pos\n",
    "            if maze[pos] == MAZE_FINISH:\n",
    "                print(\"MADE IT TO THE END OF THE MAZE.\")\n",
    "                break\n",
    "            elif maze[pos] == MAZE_WALL:\n",
    "                print(\"LOSE: HIT WALL\")\n",
    "                break\n",
    "            elif maze[pos] == HARVESTABLE_CROP:\n",
    "                print(\"HARVESTED A CROP\")\n",
    "                maze[pos] = MAZE_EMPTY_SPACE\n",
    "            elif maze[pos] == HUMAN:\n",
    "                print(\"HARVESTED A HUMAN!!!!!\")\n",
    "                maze[pos] = MAZE_EMPTY_SPACE\n",
    "        else:\n",
    "            print(\"LOSE: OUTSIDE MAZE\")\n",
    "            break\n",
    "        depth -= 1\n",
    "        if depth == 0:\n",
    "            print(\"LOSE: TOO DEEP\")\n",
    "            break\n",
    "    print(f\"{move_list=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "random.seed(1007)\n",
    "\n",
    "torch.manual_seed(1007)\n",
    "\n",
    "# Again, this experiment is particularly sensitive to what the initial weights are so we're initializing our neural\n",
    "# net from a set of known weights.\n",
    "model = NeuralNetwork()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "plot_maze(example_maze, MAZE_WIDTH)\n",
    "ascii_maze(example_maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db249f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84203e17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def plot_policy(model, maze):\n",
    "    dirs = {\n",
    "        0: '↑',\n",
    "        1: '↓',\n",
    "        2: '←',\n",
    "        3: '→',\n",
    "    }\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(-maze, 'Greys')\n",
    "    for pos_as_list in ((maze != MAZE_WALL) & (maze != MAZE_FINISH)).nonzero().tolist():\n",
    "        pos = tuple(pos_as_list)\n",
    "        q = model(to_input(maze, pos))\n",
    "        action = int(torch.argmax(q).detach().cpu().item())\n",
    "        dir = dirs[action]\n",
    "        letter_label = string_repr_of_item(maze[pos].item())\n",
    "        ax.text(pos[1] - 0.3, pos[0] + 0.3, dir + letter_label)  # center arrows in empty slots\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "play(model, example_maze, pos=(0, 0))\n",
    "plot_policy(model, example_maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here are some interesting representative examples of the \n",
    "\n",
    "good_example_0 = torch.tensor(\n",
    "    [[1., 1., 1., 0., 3., 1., 1.],\n",
    "     [0., 0., 1., 0., 0., 0., 1.],\n",
    "     [2., 0., 1., 0., 1., 1., 1.],\n",
    "     [1., 0., 1., 0., 1., 0., 1.],\n",
    "     [1., 0., 1., 1., 1., 0., 1.],\n",
    "     [1., 0., 0., 0., 0., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 1., -1.]])\n",
    "\n",
    "good_example_1 = torch.tensor(\n",
    "    [[1., 0., 2., 1., 1., 1., 1.],\n",
    "     [1., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 0., 1.],\n",
    "     [0., 0., 0., 0., 0., 0., 1.],\n",
    "     [1., 1., 1., 1., 2., 0., 1.],\n",
    "     [1., 0., 1., 0., 0., 0., 1.],\n",
    "     [3., 0., 1., 1., 1., 1., -1.]])\n",
    "\n",
    "good_example_2 = torch.tensor(\n",
    "    [[1., 0., 3., 1., 1., 1., 1.],\n",
    "     [1., 0., 0., 0., 0., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 0., 1.],\n",
    "     [0., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 2., 0., 1., 1., 1.],\n",
    "     [1., 0., 0., 0., 0., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 1., -1.]])\n",
    "\n",
    "good_example_3 = torch.tensor(\n",
    "    [[1., 0., 1., 1., 1., 1., 1.],\n",
    "     [1., 0., 1., 0., 0., 0., 1.],\n",
    "     [1., 1., 1., 0., 3., 0., 1.],\n",
    "     [0., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 0., 1.],\n",
    "     [1., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 2., 0., 1., 1., -1.]])\n",
    "\n",
    "reasonable_ish_example_0 = torch.tensor(\n",
    "    [[1., 1., 1., 1., 1., 0., 3.],\n",
    "     [0., 0., 0., 0., 1., 0., 1.],\n",
    "     [2., 0., 1., 1., 1., 0., 1.],\n",
    "     [1., 0., 1., 0., 0., 0., 1.],\n",
    "     [1., 0., 1., 1., 1., 0., 1.],\n",
    "     [1., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 1., -1.]])\n",
    "\n",
    "bad_example_0 = torch.tensor(\n",
    "    [[1., 1., 1., 1., 1., 1., 1.],\n",
    "     [0., 0., 0., 0., 0., 0., 1.],\n",
    "     [1., 0., 1., 0., 1., 1., 1.],\n",
    "     [1., 1., 1., 0., 1., 0., 0.],\n",
    "     [1., 0., 1., 0., 1., 1., 1.],\n",
    "     [1., 0., 1., 0., 0., 0., 1.],\n",
    "     [3., 0., 1., 1., 1., 1., -1.]])\n",
    "\n",
    "bad_example_1 = torch.tensor(\n",
    "    [[1., 0., 3., 1., 1., 1., 1.],\n",
    "     [1., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 1., 0., 1., 0., 1.],\n",
    "     [0., 0., 1., 1., 1., 0., 1.],\n",
    "     [2., 0., 1., 0., 1., 0., 1.],\n",
    "     [1., 0., 0., 0., 0., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 1., -1.]])\n",
    "\n",
    "bad_example_2 = torch.tensor(\n",
    "    [[1., 0., 1., 1., 1., 1., 3.],\n",
    "     [1., 0., 0., 1., 0., 0., 0.],\n",
    "     [1., 0., 1., 1., 1., 1., 1.],\n",
    "     [1., 0., 1., 0., 0., 0., 1.],\n",
    "     [1., 0., 2., 0., 1., 1., 1.],\n",
    "     [1., 0., 0., 0., 1., 0., 1.],\n",
    "     [1., 1., 1., 1., 1., 0., -1.]])\n",
    "\n",
    "okayish_examples = [good_example_0, good_example_1, good_example_2, good_example_3, reasonable_ish_example_0]\n",
    "bad_examples = [bad_example_0, bad_example_1, bad_example_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db968d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now \n",
    "\n",
    "random.seed(1007)\n",
    "\n",
    "torch.manual_seed(1007)\n",
    "\n",
    "# Again, this experiment is particularly sensitive to what the initial weights are so we're initializing our neural\n",
    "# net from a set of known weights.\n",
    "model = NeuralNetwork()\n",
    "\n",
    "model.load_state_dict(torch.load('initial-weights-new.pt.v2'))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "plot_maze(example_maze, MAZE_WIDTH)\n",
    "ascii_maze(example_maze)\n",
    "\n",
    "train(model)\n",
    "\n",
    "torch.save(model.state_dict(), 'final-weights.pt')\n",
    "\n",
    "for example in okayish_examples:\n",
    "    play(model, example, pos=(0, 0))\n",
    "    plot_policy(model, example)\n",
    "\n",
    "for example in bad_examples:\n",
    "    play(model, example, pos=(0, 0))\n",
    "    plot_policy(model, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88ecaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
